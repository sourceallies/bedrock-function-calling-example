{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_profile_name = \"dev\"\n",
    "aws_region = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "boto3.setup_default_session(profile_name=aws_profile_name)\n",
    "BEDROCK_CLIENT = boto3.client(\"bedrock-runtime\", aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Without Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "chat_model = ChatBedrock(\n",
    "    client=BEDROCK_CLIENT,\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    model_kwargs={\"max_tokens\": 1000, \"temperature\": 0.2, \"top_p\": 0.9},\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. If asked a question you do not know the answer to, simply respond with 'I don't know' instead of guessing.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "message = chain.invoke({\"question\": \"What is the temperature in Urbandale, IA today?\"})\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp(city: str, state: str):\n",
    "    \"\"\"\n",
    "    Would normally call out to an API or possibly another\n",
    "    GenAI solution. However, in this case, just return\n",
    "    that is is 72 degrees no matter where we are, so\n",
    "    we can pretend we live in a wonderful climate.\n",
    "    \"\"\"\n",
    "    print(f\"get_temp tool called with city: {city} and state: {state}\")\n",
    "    return \"72 Degrees Fahrenheit\"\n",
    "\n",
    "\n",
    "get_temp_description = \"\"\"\n",
    "<tool_description>\n",
    "    <tool_name>get_temp</tool_name>\n",
    "    <description>\n",
    "        Returns temperature data for a given city and state.\n",
    "    </description>\n",
    "    <parameters>\n",
    "        <parameter>\n",
    "            <name>city</name>\n",
    "            <type>string</type>\n",
    "            <description>The city as a string</description>\n",
    "        </parameter>\n",
    "        <parameter>\n",
    "            <name>state</name>\n",
    "            <type>string</type>\n",
    "            <description>The state as a string</description>\n",
    "        </parameter>\n",
    "    </parameters>\n",
    "</tool_description>\n",
    "\"\"\".strip()\n",
    "\n",
    "all_tool_descriptions = \"\\n\".join([get_temp_description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions when translating to and from model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(tool_name, parameters):\n",
    "    func = globals()[tool_name]\n",
    "    output = func(**parameters)\n",
    "    return output\n",
    "\n",
    "\n",
    "def format_result(tool_name, output):\n",
    "    return f\"\"\"\n",
    "<function_results>\n",
    "<result>\n",
    "<tool_name>{tool_name}</tool_name>\n",
    "<stdout>\n",
    "{output}\n",
    "</stdout>\n",
    "</result>\n",
    "</function_results>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remake model with new stop sequence added\n",
    "\n",
    "## Add tools definitions to system prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call_tag_start = \"<function_calls>\"\n",
    "function_call_tag_end = \"</function_calls>\"\n",
    "\n",
    "chat_model = ChatBedrock(\n",
    "    client=BEDROCK_CLIENT,\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop_sequences\": [\"Human: \", function_call_tag_end],\n",
    "    }\n",
    ")\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "            You are a helpful assistant. If asked a question you do not know the answer to, simply respond with 'I don't know' instead of guessing.\n",
    "            \n",
    "            In this environment you have access to a set of tools you can use to answer the user's question.\n",
    "\n",
    "            You may call them like this. Only invoke one function at a time and wait for the results before invoking another function:\n",
    "            <function_calls>\n",
    "                <invoke>\n",
    "                    <tool_name>$TOOL_NAME</tool_name>\n",
    "                    <parameters>\n",
    "                        <$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "                        ...\n",
    "                    </parameters>\n",
    "                </invoke>\n",
    "            </function_calls>\n",
    "\n",
    "            Here are the tools available:\n",
    "            <tools>\n",
    "            {tools_string}\n",
    "            </tools>\n",
    "        \"\"\".strip(),\n",
    "    ),\n",
    "    (\"human\", \"{question}\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke chain on a loop. If what it returns ends because of the new stop reason instead of the normal one, cut out the invoke portion, and use the info to call our function. Then add the AI message and a new user message with the tool call result and call again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: For some reason, langchain does not return stop_reason, which I feel like it should. Instead, we just have to check if we ended in </invoke> to see if it ended in a function call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTION CALL MESSAGE:\n",
      "To answer your question about the temperature in Urbandale, IA today, I'll need to use the get_temp tool to retrieve that information. Let me do that for you.\n",
      "\n",
      "<function_calls>\n",
      "    <invoke>\n",
      "        <tool_name>get_temp</tool_name>\n",
      "        <parameters>\n",
      "            <city>Urbandale</city>\n",
      "            <state>IA</state>\n",
      "        </parameters>\n",
      "    </invoke>\n",
      "\n",
      "get_temp tool called with city: Urbandale and state: IA\n",
      "TOOL RESPONSE:\n",
      "\n",
      "<function_results>\n",
      "<result>\n",
      "<tool_name>get_temp</tool_name>\n",
      "<stdout>\n",
      "72 Degrees Fahrenheit\n",
      "</stdout>\n",
      "</result>\n",
      "</function_results>\n",
      "\n",
      "FINAL ANSWER:\n",
      "Based on the information I've retrieved, the temperature in Urbandale, IA today is 72 degrees Fahrenheit.\n",
      "\n",
      "This is a comfortable temperature, typical of a pleasant day. It's neither too hot nor too cold, making it suitable for various outdoor activities. Remember that temperatures can change throughout the day, so this might be the current temperature or an average for the day.\n",
      "\n",
      "Is there anything else you'd like to know about the weather in Urbandale or any other location?\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "\n",
    "while True:\n",
    "    resp = chain.invoke(\n",
    "        {\n",
    "            \"question\": \"What is the temperature in Urbandale, IA today?\",\n",
    "            \"tools_string\": all_tool_descriptions,\n",
    "        }\n",
    "    )\n",
    "    message = str(resp.content)\n",
    "\n",
    "    if function_call_tag_start in message:\n",
    "        print(\"FUNCTION CALL MESSAGE:\")\n",
    "        print(message)\n",
    "        # Find the function_calls xml portion of the response.\n",
    "        start_index = message.find(function_call_tag_start) + len(\n",
    "            function_call_tag_start\n",
    "        )\n",
    "        calls = message[start_index:-1]\n",
    "\n",
    "        # Turn that XML into dictionaries\n",
    "        dict_calls = xmltodict.parse(calls)\n",
    "\n",
    "        # Call the appropriate tools\n",
    "        call = dict_calls[\"invoke\"]\n",
    "        answer = call_function(**call)\n",
    "\n",
    "        # Format the result of the tool call into XML so the model\n",
    "        # can parse it better\n",
    "        formatted_answer = format_result(call[\"tool_name\"], answer)\n",
    "        print(\"TOOL RESPONSE:\")\n",
    "        print(formatted_answer)\n",
    "\n",
    "        # \"Continue\" the conversation with the model from where we\n",
    "        # left off by updating our messages to include the tool response\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            messages + [resp, (\"human\", formatted_answer)]\n",
    "        )\n",
    "        chain = prompt | chat_model\n",
    "    else:\n",
    "        print(\"FINAL ANSWER:\")\n",
    "        print(resp.content)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
